# Summary of exercises

This provides a "table of contents" of the exercises in each set of lecture notes for easy lookup.

## Linear regression (Lecture 2)

1. Show why the gradient is "the direction and rate of maximum increase of a function".
2. Given a differentiable convex function `f`, the graph lies above all of its tangent planes.
   1. Give three examples of convex functions.
   2. Show that a local minimum of a strictly convex function is a global minimum.
   3. If you find a minimum of the mean-squared error of *linear regression*, is it the global minimum?
   4. Is the sum of two convex functions convex?
3. One dimensional gradient descent (link provided):
   1. Explain the behaviour of gradient descent with Initial slope = 0.5; Learning rate = 0.14; Batch size = 20; Number of iterations = 25. Suggest strategies to improve convergence.
4. The mean-squared error can be written in matrix form as `J` = ... Can you find the optimal set of parameters `\theta` without gradient descent?
